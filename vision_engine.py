<<<<<<< HEAD
# -*- coding: utf-8 -*-
=======
>>>>>>> aa72dfb (Initial project setup: WebPedPok YouTube video analysis and content intelligence system)
import cv2
import numpy as np
from typing import Tuple, Optional, List

class VisionEngine:
    """Component 3: Computer Vision (Auto-Reframe)
    
    à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™: à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹ƒà¸šà¸«à¸™à¹‰à¸² à¹à¸¥à¸° auto-crop à¸§à¸´à¸”à¸µà¹‚à¸­à¹€à¸›à¹‡à¸™à¹à¸™à¸§à¸•à¸±à¹‰à¸‡ (9:16)
    à¹‚à¸”à¸¢à¹ƒà¸«à¹‰à¹ƒà¸šà¸«à¸™à¹‰à¸²à¸„à¸™à¸žà¸¹à¸”à¸­à¸¢à¸¹à¹ˆà¸à¸¶à¹ˆà¸‡à¸à¸¥à¸²à¸‡/à¸šà¸™
    """
    
    def __init__(self, confidence: float = 0.5):
        """
        Args:
            confidence: à¸„à¸§à¸²à¸¡à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸±à¹ˆà¸™à¹ƒà¸™à¸à¸²à¸£à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹ƒà¸šà¸«à¸™à¹‰à¸² (0-1)
        """
        try:
            import mediapipe as mp
            # Create face detection object
            face_detection = mp.solutions.face_detection.FaceDetection(
                model_selection=1,  # model_selection=1 à¸ªà¸³à¸«à¸£à¸±à¸šà¸§à¸´à¸”à¸µà¹‚à¸­à¹à¸ªà¸‡
                min_detection_confidence=confidence
            )
            self.mp_face_detection = face_detection
            print(f"ðŸ”§ à¸à¸³à¸¥à¸±à¸‡à¸ªà¸£à¹‰à¸²à¸‡ VisionEngine...")
            print(f"   - Confidence: {confidence}")
            print(f"   âœ… MediaPipe Face Detection loaded")
        except (ImportError, AttributeError) as e:
            print("âŒ à¹„à¸¡à¹ˆà¸žà¸š 'mediapipe' package à¸«à¸£à¸·à¸­ import error")
            print(f"   Error: {e}")
            print("   à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡à¸”à¹‰à¸§à¸¢: pip install mediapipe")
            raise
    
    def get_face_center(self, frame: np.ndarray) -> Tuple[float, float, bool]:
        """à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹ƒà¸šà¸«à¸™à¹‰à¸² à¹à¸¥à¸°à¸„à¸·à¸™à¸ˆà¸¸à¸”à¸à¸¶à¹ˆà¸‡à¸à¸¥à¸²à¸‡
        
        Args:
            frame: à¸ à¸²à¸ž frame (BGR format à¹€à¸«à¸¡à¸·à¸­à¸™à¸ˆà¸²à¸ OpenCV)
            
        Returns:
            (x_center, y_center, detected): 
            - x_center: à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡ X à¸‚à¸­à¸‡à¸ˆà¸¸à¸”à¸à¸¶à¹ˆà¸‡à¸à¸¥à¸²à¸‡à¹ƒà¸šà¸«à¸™à¹‰à¸² (0-1 relative)
            - y_center: à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡ Y à¸‚à¸­à¸‡à¸ˆà¸¸à¸”à¸à¸¶à¹ˆà¸‡à¸à¸¥à¸²à¸‡à¹ƒà¸šà¸«à¸™à¹‰à¸² (0-1 relative)
            - detected: True à¸«à¸²à¸à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹ƒà¸šà¸«à¸™à¹‰à¸²
        """
        # à¹à¸›à¸¥à¸‡ BGR à¹€à¸›à¹‡à¸™ RGB
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹ƒà¸šà¸«à¸™à¹‰à¸²
        results = self.mp_face_detection.process(frame_rgb)
        
        if results.detections:
            # à¹ƒà¸Šà¹‰à¹ƒà¸šà¸«à¸™à¹‰à¸²à¸•à¸±à¸§à¹à¸£à¸ (à¸›à¸à¸•à¸´à¸„à¸·à¸­ person principal)
            bbox = results.detections[0].location_data.relative_bounding_box
            
            x_center = bbox.xmin + (bbox.width / 2)
            y_center = bbox.ymin + (bbox.height / 2)
            
            return x_center, y_center, True
        
        # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¹€à¸ˆà¸­à¹ƒà¸šà¸«à¸™à¹‰à¸² à¸„à¸·à¸™ default (à¸à¸¶à¹ˆà¸‡à¸à¸¥à¸²à¸‡)
        return 0.5, 0.5, False
    
    def get_multiple_faces(self, frame: np.ndarray) -> List[Tuple[float, float]]:
        """à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¸«à¸¥à¸²à¸¢à¹ƒà¸šà¸«à¸™à¹‰à¸² à¸„à¸·à¸™à¸£à¸²à¸¢à¸à¸²à¸£à¸ˆà¸¸à¸”à¸à¸¶à¹ˆà¸‡à¸à¸¥à¸²à¸‡
        
        Args:
            frame: à¸ à¸²à¸ž frame (BGR format)
            
        Returns:
            List[(x_center, y_center)]: à¸£à¸²à¸¢à¸à¸²à¸£à¸ˆà¸¸à¸”à¸à¸¶à¹ˆà¸‡à¸à¸¥à¸²à¸‡à¸‚à¸­à¸‡à¹ƒà¸šà¸«à¸™à¹‰à¸²
        """
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.mp_face_detection.process(frame_rgb)
        
        face_centers = []
        if results.detections:
            for detection in results.detections:
                bbox = detection.location_data.relative_bounding_box
                x_center = bbox.xmin + (bbox.width / 2)
                y_center = bbox.ymin + (bbox.height / 2)
                face_centers.append((x_center, y_center))
        
        return face_centers
    
    def calculate_crop_window_9_16(
        self,
        frame_width: int,
        frame_height: int,
        face_x: float,
        face_y: float,
        padding_vertical: float = 0.2
    ) -> Tuple[int, int, int, int]:
        """à¸„à¸³à¸™à¸§à¸“ crop window à¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™ 9:16 à¹‚à¸”à¸¢à¹ƒà¸«à¹‰à¹ƒà¸šà¸«à¸™à¹‰à¸²à¸­à¸¢à¸¹à¹ˆà¸à¸¥à¸²à¸‡
        
        Args:
            frame_width: à¸„à¸§à¸²à¸¡à¸à¸§à¹‰à¸²à¸‡à¸‚à¸­à¸‡ frame à¸•à¹‰à¸™à¸‰à¸šà¸±à¸š
            frame_height: à¸„à¸§à¸²à¸¡à¸ªà¸¹à¸‡à¸‚à¸­à¸‡ frame à¸•à¹‰à¸™à¸‰à¸šà¸±à¸š
            face_x: à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡ X à¸‚à¸­à¸‡à¹ƒà¸šà¸«à¸™à¹‰à¸² (0-1)
            face_y: à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡ Y à¸‚à¸­à¸‡à¹ƒà¸šà¸«à¸™à¹‰à¸² (0-1)
            padding_vertical: padding à¹€à¸žà¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡à¸ˆà¸²à¸ top (0.2 = 20% à¸‚à¸­à¸‡à¸„à¸§à¸²à¸¡à¸ªà¸¹à¸‡)
            
        Returns:
            (x1, y1, x2, y2): à¸žà¸´à¸à¸±à¸” crop window (pixels)
        """
        # à¸­à¸±à¸•à¸£à¸²à¸ªà¹ˆà¸§à¸™ 9:16 (width:height)
        target_ratio = 9 / 16
        
        # à¸à¸³à¸«à¸™à¸”à¸„à¸§à¸²à¸¡à¸à¸§à¹‰à¸²à¸‡à¸‚à¸­à¸‡ crop window
        # à¹ƒà¸Šà¹‰à¸„à¸§à¸²à¸¡à¸ªà¸¹à¸‡ frame à¸•à¹‰à¸™à¸‰à¸šà¸±à¸šà¹€à¸›à¹‡à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡
        target_width = int(frame_height * target_ratio)
        target_height = frame_height
        
        # à¸–à¹‰à¸² target_width > frame_width à¹ƒà¸«à¹‰à¸›à¸£à¸±à¸š
        if target_width > frame_width:
            target_width = frame_width
            target_height = int(target_width / target_ratio)
        
        # à¸„à¸³à¸™à¸§à¸“à¸ˆà¸¸à¸”à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™ à¹‚à¸”à¸¢à¹ƒà¸«à¹‰à¹ƒà¸šà¸«à¸™à¹‰à¸²à¸­à¸¢à¸¹à¹ˆà¸•à¸£à¸‡à¸à¸¥à¸²à¸‡à¸ªà¸¹à¸‡
        # à¹à¸•à¹ˆà¸­à¸¢à¸¹à¹ˆà¸•à¸²à¸¡ face_x horizontally
        # à¹à¸¥à¸° offset à¸‚à¸¶à¹‰à¸™à¸¡à¸²à¹€à¸¥à¹‡à¸à¸™à¹‰à¸­à¸¢à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸¡à¸µà¸žà¸·à¹‰à¸™à¸—à¸µà¹ˆà¸šà¸™
        
        # Horizontal: à¸ˆà¸±à¸”à¸¨à¸¹à¸™à¸¢à¹Œà¸à¸¥à¸²à¸‡ face
        center_x = int(face_x * frame_width)
        x1 = max(0, center_x - target_width // 2)
        x2 = min(frame_width, x1 + target_width)
        
        # à¸›à¸£à¸±à¸šà¸–à¹‰à¸²à¹€à¸à¸´à¸™à¸‚à¸­à¸š
        if x2 > frame_width:
            x2 = frame_width
            x1 = max(0, x2 - target_width)
        if x1 < 0:
            x1 = 0
            x2 = min(frame_width, x1 + target_width)
        
        # Vertical: à¹ƒà¸«à¹‰à¹ƒà¸šà¸«à¸™à¹‰à¸²à¸­à¸¢à¸¹à¹ˆà¸ªà¸¹à¸‡à¸‚à¸¶à¹‰à¸™à¸¡à¸² (padding_vertical)
        target_y = int(frame_height * padding_vertical)
        y1 = max(0, int(face_y * frame_height) - target_y)
        y2 = min(frame_height, y1 + target_height)
        
        # à¸›à¸£à¸±à¸šà¸–à¹‰à¸²à¹€à¸à¸´à¸™à¸‚à¸­à¸š
        if y2 > frame_height:
            y2 = frame_height
            y1 = max(0, y2 - target_height)
        if y1 < 0:
            y1 = 0
            y2 = min(frame_height, y1 + target_height)
        
        return x1, y1, x2, y2
    
    def crop_frame_9_16(
        self,
        frame: np.ndarray,
        face_x: float,
        face_y: float,
        padding_vertical: float = 0.2
    ) -> np.ndarray:
        """à¸•à¸±à¸” frame à¹ƒà¸«à¹‰à¹€à¸›à¹‡à¸™ 9:16 à¹‚à¸”à¸¢à¹ƒà¸«à¹‰à¹ƒà¸šà¸«à¸™à¹‰à¸²à¸­à¸¢à¸¹à¹ˆà¸à¸¥à¸²à¸‡
        
        Args:
            frame: à¸ à¸²à¸ž frame (BGR)
            face_x: à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡ X à¸‚à¸­à¸‡à¹ƒà¸šà¸«à¸™à¹‰à¸² (0-1)
            face_y: à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡ Y à¸‚à¸­à¸‡à¹ƒà¸šà¸«à¸™à¹‰à¸² (0-1)
            padding_vertical: padding from top
            
        Returns:
            cropped_frame: à¸ à¸²à¸žà¸—à¸µà¹ˆà¸•à¸±à¸”à¹à¸¥à¹‰à¸§
        """
        h, w = frame.shape[:2]
        x1, y1, x2, y2 = self.calculate_crop_window_9_16(w, h, face_x, face_y, padding_vertical)
        
        return frame[y1:y2, x1:x2]
    
    def process_video_samples(self, video_path: str, sample_frames: int = 5) -> dict:
        """à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸§à¸´à¸”à¸µà¹‚à¸­ à¹€à¸à¹‡à¸š sample face positions
        
        Args:
            video_path: à¹€à¸ªà¹‰à¸™à¸—à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œà¸§à¸´à¸”à¸µà¹‚à¸­
            sample_frames: à¸ˆà¸³à¸™à¸§à¸™ frame à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£
            
        Returns:
            dict: à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ
        """
        print(f"ðŸ‘ï¸  4. à¸à¸³à¸¥à¸±à¸‡à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸§à¸´à¸”à¸µà¹‚à¸­à¹€à¸žà¸·à¹ˆà¸­à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹ƒà¸šà¸«à¸™à¹‰à¸²...")
        
        cap = cv2.VideoCapture(video_path)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        face_positions = []
        
        # Sample frames à¸—à¸µà¹ˆà¸£à¸°à¸¢à¸°à¸«à¹ˆà¸²à¸‡à¹€à¸—à¹ˆà¸²à¸à¸±à¸™
        sample_indices = np.linspace(0, frame_count - 1, sample_frames, dtype=int)
        
        for frame_idx in sample_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            
            if ret:
                x_center, y_center, detected = self.get_face_center(frame)
                time_sec = frame_idx / fps if fps > 0 else 0
                
                face_positions.append({
                    "frame": frame_idx,
                    "time_sec": time_sec,
                    "face_x": x_center,
                    "face_y": y_center,
                    "detected": detected
                })
        
        cap.release()
        
        print(f"   âœ… à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸ªà¸³à¹€à¸£à¹‡à¸ˆ ({len(face_positions)} samples)")
        print(f"   Total frames: {frame_count}, FPS: {fps:.1f}")
        
        return {
            "video_path": video_path,
            "frame_count": frame_count,
            "fps": fps,
            "face_positions": face_positions,
            "detected_count": sum(1 for p in face_positions if p["detected"])
        }
